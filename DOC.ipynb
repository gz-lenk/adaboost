{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:14:52.029642Z",
     "start_time": "2025-01-14T08:14:52.025342Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T06:48:30.514704Z",
     "start_time": "2025-01-14T06:48:30.504254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CamouflageDataset(Dataset):\n",
    "    def __init__(self, image_dir, gt_dir, image_transform=None, gt_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): 普通图像文件夹路径。\n",
    "            gt_dir (str): 掩码图像文件夹路径。\n",
    "            image_transform (callable, optional): 图像的变换操作。\n",
    "            gt_transform (callable, optional): 掩码的变换操作。\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.gt_transform = gt_transform\n",
    "        self.image_names = os.listdir(image_dir)  # 获取所有图像文件名\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载普通图像\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # 确保图像是 RGB 格式\n",
    "\n",
    "        # 加载对应的掩码图像\n",
    "        gt_name = image_name.replace(\".jpg\", \".png\")  # 假设掩码图像文件名与普通图像文件名一致，只是扩展名不同\n",
    "        gt_path = os.path.join(self.gt_dir, gt_name)\n",
    "        gt = Image.open(gt_path).convert(\"L\")  # 转换为灰度图像\n",
    "\n",
    "        # 将图像和掩码转换为 NumPy 数组\n",
    "        image = np.array(image)\n",
    "        gt = np.array(gt)\n",
    "\n",
    "        # 将掩码二值化（0 表示背景，1 表示目标）\n",
    "        gt = (gt > 128).astype(np.float32)\n",
    "\n",
    "        # 应用变换（如果有）\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.gt_transform:\n",
    "            gt = self.gt_transform(gt)\n",
    "\n",
    "        # 返回图像和掩码\n",
    "        return image, gt\n",
    "\n",
    "# 定义图像变换\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将图像转换为 PyTorch 张量\n",
    "    transforms.Resize((300,400)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 归一化\n",
    "])\n",
    "\n",
    "# 定义掩码变换\n",
    "gt_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 将掩码转换为 PyTorch 张量\n",
    "    transforms.Resize((300,400)),\n",
    "    transforms.Lambda(lambda x: (x > 0.5).float())  # 将掩码二值化\n",
    "])\n",
    "\n",
    "# 定义数据集路径\n",
    "train_image_dir = \"datasets/dataset/train/image\"\n",
    "train_gt_dir = \"datasets/dataset/train/GT\"\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = CamouflageDataset(image_dir=train_image_dir, gt_dir=train_gt_dir, image_transform=image_transform, gt_transform=gt_transform)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 定义测试数据集路径\n",
    "test_image_dir = \"datasets/dataset/test/image\"\n",
    "test_gt_dir = \"datasets/dataset/test/GT\"\n",
    "\n",
    "# 创建测试数据集实例\n",
    "test_dataset = CamouflageDataset(image_dir=test_image_dir, gt_dir=test_gt_dir, image_transform=image_transform, gt_transform=gt_transform)\n",
    "\n",
    "# 创建测试 DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "a6c7ac6d4cfda5e8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T08:36:01.163278Z",
     "start_time": "2025-01-14T08:36:01.141413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# show the data \n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, masks = next(dataiter)\n",
    "fig, ax = plt.subplots(2, len(images), figsize=(10, 5))\n",
    "for i in range(len(images)*2):\n",
    "    if i<len(images)-1 :\n",
    "        ax[i].imshow(images[i])\n",
    "    else:\n",
    "        ax[i].imshow(masks[i-len(images)])"
   ],
   "id": "29bc6766fb088f60",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m     plt\u001B[38;5;241m.\u001B[39mimshow(np\u001B[38;5;241m.\u001B[39mtranspose(npimg, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m)))\n\u001B[0;32m      6\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m----> 8\u001B[0m dataiter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(train_loader)\n\u001B[0;32m      9\u001B[0m images, masks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(dataiter)\n\u001B[0;32m     10\u001B[0m fig, ax \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mlen\u001B[39m(images), figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not callable"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T07:34:02.702243Z",
     "start_time": "2025-01-14T07:34:02.694183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 特征提取\n",
    "# 提取颜色直方图特征\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# 提取LBP纹理特征\n",
    "def extract_lbp_feature(image, radius=3, n_points=24):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method=\"uniform\")\n",
    "    (hist, _) = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)  # 归一化\n",
    "    return hist\n",
    "\n",
    "# 提取每张图像的特征\n",
    "def extract_features(images, features_list):\n",
    "    for image in images:\n",
    "        image = image.permute(1, 2, 0).cpu().numpy()\n",
    "        # 转换为OpenCV格式\n",
    "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # 提取颜色直方图特征\n",
    "        hist_feature = extract_color_histogram(image_bgr)\n",
    "        # 提取LBP纹理特征\n",
    "        lbp_feature = extract_lbp_feature(image_bgr)\n",
    "        # 合并特征\n",
    "        combined_feature = np.hstack([hist_feature, lbp_feature])\n",
    "        features_list.append(combined_feature)\n",
    "    return features_list\n",
    "\n",
    "# 提取标签\n",
    "def extract_labels(masks, labels_list):\n",
    "    for mask in masks:\n",
    "        label = (mask > 0).to(torch.bool).flatten().numpy()\n",
    "        labels_list.append(label)\n",
    "    return labels_list"
   ],
   "id": "7d6567edec42e179",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义网络架构\n",
    "class FCNNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_channels):\n",
    "        super(FCNNetwork, self).__init__()\n",
    "        # 定义卷积层\n",
    "        self.conv1 = nn.Conv2d(input_dim, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, output_channels, kernel_size=1)\n",
    "        # 定义上采样层\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入形状为 (batch_size, D)，需要扩展为 (batch_size, D, 1, 1)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        # 前向传播\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ],
   "id": "dce0bc55137bb6e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T07:33:56.776279Z",
     "start_time": "2025-01-14T07:31:09.425164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "features_list=[]\n",
    "labels_list=[]\n",
    "for iter, (images, masks) in enumerate(train_loader, start=1):\n",
    "    print(f\"Iteration: {iter}\")\n",
    "    features_list = extract_features(images,features_list)\n",
    "    labels_list = extract_labels(masks, labels_list)"
   ],
   "id": "6a4dc0f4af53f29b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T07:45:59.924327Z",
     "start_time": "2025-01-14T07:45:59.781633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features_list = np.array(features_list)\n",
    "labels_list = np.array(labels_list)\n",
    "print(features_list.shape)\n",
    "print(labels_list.shape)\n",
    "labels_flat = labels_list.reshape(-1)"
   ],
   "id": "eece8f77546373d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2900, 538)\n",
      "(2900, 120000)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T07:55:55.389406Z",
     "start_time": "2025-01-14T07:55:55.343730Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "36e1be9e4e9952f5",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 120000]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(features_list\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m----> 2\u001B[0m     clf\u001B[38;5;241m.\u001B[39mfit(features_list[i]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), labels_list[i])\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch-general\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch-general\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:133\u001B[0m, in \u001B[0;36mBaseWeightBoosting.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Build a boosted classifier/regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \n\u001B[0;32m    114\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m    Fitted estimator.\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    132\u001B[0m _raise_for_unsupported_routing(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, sample_weight\u001B[38;5;241m=\u001B[39msample_weight)\n\u001B[1;32m--> 133\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[0;32m    134\u001B[0m     X,\n\u001B[0;32m    135\u001B[0m     y,\n\u001B[0;32m    136\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    137\u001B[0m     ensure_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    138\u001B[0m     allow_nd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    139\u001B[0m     dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    140\u001B[0m     y_numeric\u001B[38;5;241m=\u001B[39mis_regressor(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    141\u001B[0m )\n\u001B[0;32m    143\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m _check_sample_weight(\n\u001B[0;32m    144\u001B[0m     sample_weight, X, np\u001B[38;5;241m.\u001B[39mfloat64, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, only_non_negative\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    145\u001B[0m )\n\u001B[0;32m    146\u001B[0m sample_weight \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m sample_weight\u001B[38;5;241m.\u001B[39msum()\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch-general\\Lib\\site-packages\\sklearn\\base.py:650\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    648\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[0;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 650\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    651\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch-general\\Lib\\site-packages\\sklearn\\utils\\validation.py:1320\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1301\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1302\u001B[0m     X,\n\u001B[0;32m   1303\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1315\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1316\u001B[0m )\n\u001B[0;32m   1318\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[1;32m-> 1320\u001B[0m check_consistent_length(X, y)\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\.conda\\envs\\pytorch-general\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    455\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 457\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    458\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    459\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[0;32m    460\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [1, 120000]"
     ]
    }
   ],
   "execution_count": 40
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
